{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51504a5-8d45-4e56-bfa2-65624e8dd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisite\n",
    "# ! pip install pytorch_lightning\n",
    "# ! pip install nltk\n",
    "# !python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e222f3a6-73a9-442d-815b-b0547748a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import random\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53184ad-91bc-4f83-84b0-b1b1c9dd53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class Prep:\n",
    "    \"\"\"Preparing tokenization and frequences.\"\"\"\n",
    "    def __init__(self):\n",
    "        with open(\"./data/wiki.test.txt\") as f:\n",
    "            self.test = f.read()\n",
    "        with open(\"./data/wiki.train.txt\") as f:\n",
    "            self.train = f.read()\n",
    "        with open(\"./data/wiki.valid.txt\") as f:\n",
    "            self.valid = f.read()\n",
    "        # self.test1 = \"After release , it received downloadable content . along with an expanded edition in November of that year .\"\n",
    "        # self.test2 = \"After it received .\"\n",
    "        self.word_freqs = {\"<oov>\":1}\n",
    "\n",
    "    def tokenize(self, corpus):\n",
    "        \"\"\"\n",
    "        Tokenized the lines, remove the titles, and make it lowercase,\n",
    "        return lines list.\n",
    "        list[list[word]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create token list\n",
    "        sent_tokens = [word_tokenize(t) for t in sent_tokenize(corpus)]\n",
    "        random.shuffle(sent_tokens)\n",
    "        word_tokens = [[w.lower() for w in s] for s in sent_tokens]\n",
    "        \n",
    "        # Remove last punctuation, add <s></s>\n",
    "        word_tokens = [[\"<s>\"] + s + [\"</s>\"] if s[-1].isalnum() else [\"<s>\"] + s[:-1] + [\"</s>\"] for s in word_tokens]\n",
    "        corpus = []\n",
    "        for s in word_tokens:\n",
    "            corpus.extend(s)\n",
    "        return corpus\n",
    "    \n",
    "    def building_vocab(self, corpus):\n",
    "        \"\"\"Building vocab list from training set.\"\"\"\n",
    "        for w in tqdm(corpus):\n",
    "            # the word has already been found\n",
    "            if w in self.word_freqs:\n",
    "                self.word_freqs[w] += 1\n",
    "            # the word has not yet already been found\n",
    "            else:\n",
    "                self.word_freqs[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3d95cb-b843-4570-a2ce-e4aaee398cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\" Converts word tokens to indices, and vice versa. \"\"\"\n",
    "\n",
    "    def __init__(self, freqs, corpus, window_size):\n",
    "        super().__init__()\n",
    "        self.indix2token = tuple(freqs)\n",
    "        self.token2index = {k: v for v, k in enumerate(self.indix2token)}\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.encoded_list = []\n",
    "        self.data, self.target = self.encoding()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return torch.tensor(self.data[key]),torch.tensor(self.target[key])\n",
    "\n",
    "    def encoding(self):  \n",
    "        def retrive(key):\n",
    "            if isinstance(key, int):\n",
    "                return None\n",
    "            else:\n",
    "                return self.token2index[key]\n",
    "        encoded_list = [retrive(i) for i in self.corpus]\n",
    "        self.encoded_list = [encoded_list[i:i + self.window_size] for i in range(0, len(encoded_list), self.window_size) if len(encoded_list[i:i + self.window_size])==self.window_size]\n",
    "        data = [s[:-1] for s in self.encoded_list]\n",
    "        target = [s[1:] for s in self.encoded_list]\n",
    "        return data, target\n",
    "    \n",
    "    def decoding(self):\n",
    "        def retrive(self, key):\n",
    "            if isinstance(key, int):\n",
    "                return self.indix2token[key]\n",
    "            else:\n",
    "                return None\n",
    "        decoded_list = [[retrive(w) for w in s] for s in self.corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af18a205-5c5b-4803-8b84-a320abaeb4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2285987/2285987 [00:00<00:00, 2640678.60it/s]\n",
      "100%|██████████| 250669/250669 [00:00<00:00, 2170187.36it/s]\n",
      "100%|██████████| 287420/287420 [00:00<00:00, 2272006.89it/s]\n"
     ]
    }
   ],
   "source": [
    "p = Prep()\n",
    "# Prepare vocab\n",
    "train_corpus = p.tokenize(p.train)\n",
    "p.building_vocab(train_corpus)\n",
    "\n",
    "valid_corpus = p.tokenize(p.valid)\n",
    "p.building_vocab(valid_corpus)\n",
    "\n",
    "test_corpus = p.tokenize(p.test)\n",
    "p.building_vocab(test_corpus)\n",
    "\n",
    "word_freqs = p.word_freqs\n",
    "\n",
    "train = Vocab(word_freqs, train_corpus, 31)\n",
    "valid = Vocab(word_freqs, valid_corpus, 31)\n",
    "test = Vocab(word_freqs, test_corpus, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b312ae-8abd-43d5-b684-098352c54eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDateModule(pl.LightningDataModule):\n",
    "    \"\"\"Pytorch lightning data module.\"\"\"\n",
    "    def __init__(self, train_corpus, valid_corpus, test_corpus):\n",
    "        super().__init__()\n",
    "        self.batch_size = 20\n",
    "        self.train = train_corpus\n",
    "        self.valid = valid_corpus\n",
    "        self.test = test_corpus\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train, self.batch_size, num_workers=16, shuffle=True, drop_last=True)\n",
    "  \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.valid, self.batch_size, num_workers=16, shuffle=False, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test, self.batch_size, num_workers=16, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bfbc3-6949-4d61-8d38-1f9cc8266f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLightningModule(pl.LightningModule):\n",
    "    \"\"\"RNN module\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = 2\n",
    "        self.hidden_size = 100 #200\n",
    "        self.embedding_size = 100\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        #layers\n",
    "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.out_fc = nn.Linear(self.hidden_size, vocab_size)\n",
    "        # loss funciton\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    \n",
    "    def forward(self, data, hidden):\n",
    "        embedding = self.dropout(self.embedding(data))\n",
    "        output, hidden = self.rnn(embedding, hidden)\n",
    "        output = self.out_fc(output)\n",
    "        return output.view(-1, self.vocab_size), hidden\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=5e-1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, hidden = self.forward(x, hidden)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'perplexity': {'train': perplexity}, 'loss': {'train': loss.detach()}}\n",
    "        self.log(\"loss/train\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"perplexity/train\", perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, hidden = self.forward(x, hidden)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'perplexity': {'valid': perplexity}, 'loss': {'valid': loss.detach()}}\n",
    "        self.log(\"loss/valid\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"perplexity/valid\", perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, hidden = self.forward(x, hidden)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'perplexity': {'test': perplexity}, 'loss': {'test': loss.detach()}}\n",
    "        self.log(\"loss/test\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"perplexity/test\", perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def init_hidden(self, batch_size = 20):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6a615-a36f-4dbb-8c03-fa0bf4cf1897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Missing logger folder: ./lightning_logs/network_1\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | embedding | Embedding        | 2.9 M \n",
      "1 | rnn       | RNN              | 40.4 K\n",
      "2 | out_fc    | Linear           | 2.9 M \n",
      "3 | loss      | CrossEntropyLoss | 0     \n",
      "4 | dropout   | Dropout          | 0     \n",
      "-----------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.400    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  90%|█████████ | 3687/4091 [00:35<00:03, 104.84it/s, loss=5.8, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  90%|█████████ | 3692/4091 [00:35<00:03, 103.31it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  91%|█████████ | 3716/4091 [00:35<00:03, 103.69it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  91%|█████████▏| 3742/4091 [00:35<00:03, 104.12it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 3768/4091 [00:36<00:03, 104.54it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 3794/4091 [00:36<00:02, 104.96it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 3820/4091 [00:36<00:02, 105.38it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  94%|█████████▍| 3846/4091 [00:36<00:02, 105.79it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 3872/4091 [00:36<00:02, 106.20it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  95%|█████████▌| 3898/4091 [00:36<00:01, 106.60it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  96%|█████████▌| 3924/4091 [00:36<00:01, 107.00it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 3950/4091 [00:36<00:01, 107.40it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  97%|█████████▋| 3976/4091 [00:36<00:01, 107.81it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 4002/4091 [00:36<00:00, 108.21it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  98%|█████████▊| 4028/4091 [00:37<00:00, 108.61it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 4054/4091 [00:37<00:00, 109.00it/s, loss=5.8, v_num=0]\u001b[A\n",
      "Epoch 0: 100%|█| 4091/4091 [00:37<00:00, 109.32it/s, loss=5.8, v_num=0, loss/val\u001b[A\n",
      "Epoch 1:  90%|▉| 3687/4091 [00:35<00:03, 104.20it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|▉| 3692/4091 [00:35<00:03, 102.67it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  91%|▉| 3718/4091 [00:36<00:03, 103.10it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  92%|▉| 3744/4091 [00:36<00:03, 103.52it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  92%|▉| 3770/4091 [00:36<00:03, 103.94it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  93%|▉| 3796/4091 [00:36<00:02, 104.35it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  93%|▉| 3822/4091 [00:36<00:02, 104.77it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  94%|▉| 3848/4091 [00:36<00:02, 105.18it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  95%|▉| 3874/4091 [00:36<00:02, 105.59it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  95%|▉| 3900/4091 [00:36<00:01, 106.00it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  96%|▉| 3926/4091 [00:36<00:01, 106.40it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  97%|▉| 3952/4091 [00:37<00:01, 106.80it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  97%|▉| 3978/4091 [00:37<00:01, 107.20it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  98%|▉| 4004/4091 [00:37<00:00, 107.59it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  99%|▉| 4030/4091 [00:37<00:00, 107.99it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1:  99%|▉| 4056/4091 [00:37<00:00, 108.38it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1: 100%|▉| 4082/4091 [00:37<00:00, 108.78it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 1: 100%|█| 4091/4091 [00:37<00:00, 108.64it/s, loss=5.52, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  90%|▉| 3687/4091 [00:35<00:03, 103.49it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|▉| 3692/4091 [00:36<00:03, 101.97it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  91%|▉| 3718/4091 [00:36<00:03, 102.38it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  92%|▉| 3744/4091 [00:36<00:03, 102.80it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  92%|▉| 3770/4091 [00:36<00:03, 103.22it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  93%|▉| 3796/4091 [00:36<00:02, 103.64it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  93%|▉| 3822/4091 [00:36<00:02, 104.05it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  94%|▉| 3848/4091 [00:36<00:02, 104.45it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  95%|▉| 3874/4091 [00:36<00:02, 104.85it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  95%|▉| 3900/4091 [00:37<00:01, 105.26it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  96%|▉| 3926/4091 [00:37<00:01, 105.66it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  97%|▉| 3952/4091 [00:37<00:01, 106.06it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  97%|▉| 3978/4091 [00:37<00:01, 106.46it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  98%|▉| 4004/4091 [00:37<00:00, 106.85it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  99%|▉| 4030/4091 [00:37<00:00, 107.25it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2:  99%|▉| 4056/4091 [00:37<00:00, 107.64it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2: 100%|▉| 4082/4091 [00:37<00:00, 108.04it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 2: 100%|█| 4091/4091 [00:37<00:00, 107.93it/s, loss=5.42, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  90%|▉| 3687/4091 [00:35<00:03, 103.66it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  90%|▉| 3692/4091 [00:36<00:03, 102.14it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  91%|▉| 3718/4091 [00:36<00:03, 102.57it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  92%|▉| 3744/4091 [00:36<00:03, 102.99it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  92%|▉| 3770/4091 [00:36<00:03, 103.41it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  93%|▉| 3796/4091 [00:36<00:02, 103.83it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  93%|▉| 3822/4091 [00:36<00:02, 104.24it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  94%|▉| 3848/4091 [00:36<00:02, 104.65it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  95%|▉| 3874/4091 [00:36<00:02, 105.05it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  95%|▉| 3900/4091 [00:36<00:01, 105.44it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  96%|▉| 3926/4091 [00:37<00:01, 105.84it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  97%|▉| 3952/4091 [00:37<00:01, 106.24it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  97%|▉| 3978/4091 [00:37<00:01, 106.64it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  98%|▉| 4004/4091 [00:37<00:00, 107.03it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  99%|▉| 4030/4091 [00:37<00:00, 107.43it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3:  99%|▉| 4056/4091 [00:37<00:00, 107.82it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 3: 100%|█| 4091/4091 [00:37<00:00, 108.11it/s, loss=5.26, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  90%|▉| 3687/4091 [00:35<00:03, 104.09it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  90%|▉| 3692/4091 [00:35<00:03, 102.57it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  91%|▉| 3718/4091 [00:36<00:03, 103.00it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  92%|▉| 3744/4091 [00:36<00:03, 103.42it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  92%|▉| 3770/4091 [00:36<00:03, 103.84it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  93%|▉| 3796/4091 [00:36<00:02, 104.25it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  93%|▉| 3822/4091 [00:36<00:02, 104.66it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  94%|▉| 3848/4091 [00:36<00:02, 105.08it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  95%|▉| 3874/4091 [00:36<00:02, 105.47it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  95%|▉| 3900/4091 [00:36<00:01, 105.88it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  96%|▉| 3926/4091 [00:36<00:01, 106.28it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  97%|▉| 3952/4091 [00:37<00:01, 106.68it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  97%|▉| 3978/4091 [00:37<00:01, 107.07it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  98%|▉| 4004/4091 [00:37<00:00, 107.46it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  99%|▉| 4030/4091 [00:37<00:00, 107.85it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4:  99%|▉| 4056/4091 [00:37<00:00, 108.24it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4: 100%|▉| 4082/4091 [00:37<00:00, 108.63it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 4: 100%|█| 4091/4091 [00:37<00:00, 108.50it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  90%|▉| 3687/4091 [00:35<00:03, 102.85it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  90%|▉| 3692/4091 [00:36<00:03, 101.36it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  91%|▉| 3718/4091 [00:36<00:03, 101.79it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  92%|▉| 3744/4091 [00:36<00:03, 102.20it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  92%|▉| 3770/4091 [00:36<00:03, 102.62it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  93%|▉| 3796/4091 [00:36<00:02, 103.03it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  93%|▉| 3822/4091 [00:36<00:02, 103.43it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  94%|▉| 3848/4091 [00:37<00:02, 103.84it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  95%|▉| 3874/4091 [00:37<00:02, 104.24it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  95%|▉| 3900/4091 [00:37<00:01, 104.65it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  96%|▉| 3926/4091 [00:37<00:01, 105.05it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  97%|▉| 3952/4091 [00:37<00:01, 105.45it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  97%|▉| 3978/4091 [00:37<00:01, 105.84it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  98%|▉| 4004/4091 [00:37<00:00, 106.24it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  99%|▉| 4030/4091 [00:37<00:00, 106.63it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5:  99%|▉| 4056/4091 [00:37<00:00, 107.02it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5: 100%|▉| 4082/4091 [00:38<00:00, 107.41it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 5: 100%|█| 4091/4091 [00:38<00:00, 107.28it/s, loss=5.24, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  90%|▉| 3687/4091 [00:35<00:03, 103.63it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  90%|▉| 3692/4091 [00:36<00:03, 102.10it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  91%|▉| 3720/4091 [00:36<00:03, 102.59it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  92%|▉| 3748/4091 [00:36<00:03, 103.03it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  92%|▉| 3776/4091 [00:36<00:03, 103.48it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  93%|▉| 3804/4091 [00:36<00:02, 103.92it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  94%|▉| 3832/4091 [00:36<00:02, 104.36it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  94%|▉| 3860/4091 [00:36<00:02, 104.80it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  95%|▉| 3888/4091 [00:36<00:01, 105.23it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Validating:  50%|██████████████              | 202/404 [00:01<00:00, 222.94it/s]\u001b[A\n",
      "Epoch 6:  96%|▉| 3916/4091 [00:37<00:01, 105.65it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  96%|▉| 3944/4091 [00:37<00:01, 106.08it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  97%|▉| 3972/4091 [00:37<00:01, 106.49it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  98%|▉| 4000/4091 [00:37<00:00, 106.92it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  98%|▉| 4028/4091 [00:37<00:00, 107.34it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6:  99%|▉| 4056/4091 [00:37<00:00, 107.76it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6: 100%|▉| 4084/4091 [00:37<00:00, 108.18it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 6: 100%|█| 4091/4091 [00:37<00:00, 108.00it/s, loss=5.11, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  90%|▉| 3687/4091 [00:35<00:03, 104.24it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  90%|▉| 3696/4091 [00:35<00:03, 102.75it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  91%|▉| 3724/4091 [00:36<00:03, 103.22it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  92%|▉| 3752/4091 [00:36<00:03, 103.67it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  92%|▉| 3780/4091 [00:36<00:02, 104.12it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  93%|▉| 3808/4091 [00:36<00:02, 104.57it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  94%|▉| 3836/4091 [00:36<00:02, 105.01it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 202.79it/s]\u001b[A\n",
      "Epoch 7:  94%|▉| 3864/4091 [00:36<00:02, 105.45it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  95%|▉| 3892/4091 [00:36<00:01, 105.88it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  96%|▉| 3920/4091 [00:36<00:01, 106.32it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  97%|▉| 3948/4091 [00:36<00:01, 106.75it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  97%|▉| 3976/4091 [00:37<00:01, 107.18it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  98%|▉| 4004/4091 [00:37<00:00, 107.60it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  99%|▉| 4032/4091 [00:37<00:00, 108.03it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7:  99%|▉| 4060/4091 [00:37<00:00, 108.44it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7: 100%|▉| 4088/4091 [00:37<00:00, 108.86it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 7: 100%|█| 4091/4091 [00:37<00:00, 108.64it/s, loss=5.15, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  90%|▉| 3687/4091 [00:35<00:03, 103.89it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  90%|▉| 3696/4091 [00:36<00:03, 102.42it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  91%|▉| 3724/4091 [00:36<00:03, 102.91it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  92%|▉| 3752/4091 [00:36<00:03, 103.35it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  92%|▉| 3780/4091 [00:36<00:02, 103.78it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  93%|▉| 3808/4091 [00:36<00:02, 104.23it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  94%|▉| 3836/4091 [00:36<00:02, 104.67it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 201.79it/s]\u001b[A\n",
      "Epoch 8:  94%|▉| 3864/4091 [00:36<00:02, 105.11it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  95%|▉| 3892/4091 [00:36<00:01, 105.54it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  96%|▉| 3920/4091 [00:36<00:01, 105.97it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  97%|▉| 3948/4091 [00:37<00:01, 106.40it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  97%|▉| 3976/4091 [00:37<00:01, 106.84it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  98%|▉| 4004/4091 [00:37<00:00, 107.26it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  99%|▉| 4032/4091 [00:37<00:00, 107.68it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8:  99%|▉| 4060/4091 [00:37<00:00, 108.10it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8: 100%|▉| 4088/4091 [00:37<00:00, 108.52it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 8: 100%|█| 4091/4091 [00:37<00:00, 108.32it/s, loss=5.09, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  90%|▉| 3687/4091 [00:35<00:03, 103.48it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  90%|▉| 3696/4091 [00:36<00:03, 102.00it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  91%|▉| 3724/4091 [00:36<00:03, 102.48it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  92%|▉| 3752/4091 [00:36<00:03, 102.93it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  92%|▉| 3780/4091 [00:36<00:03, 103.37it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  93%|▉| 3808/4091 [00:36<00:02, 103.80it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  94%|▉| 3836/4091 [00:36<00:02, 104.24it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 197.71it/s]\u001b[A\n",
      "Epoch 9:  94%|▉| 3864/4091 [00:36<00:02, 104.66it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  95%|▉| 3892/4091 [00:37<00:01, 105.10it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  96%|▉| 3920/4091 [00:37<00:01, 105.53it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  97%|▉| 3948/4091 [00:37<00:01, 105.96it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  97%|▉| 3976/4091 [00:37<00:01, 106.39it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  98%|▉| 4004/4091 [00:37<00:00, 106.82it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  99%|▉| 4032/4091 [00:37<00:00, 107.24it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9:  99%|▉| 4060/4091 [00:37<00:00, 107.66it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9: 100%|▉| 4088/4091 [00:37<00:00, 108.08it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 9: 100%|█| 4091/4091 [00:37<00:00, 107.89it/s, loss=5.03, v_num=0, loss/va\u001b[A\n",
      "Epoch 10:  90%|▉| 3687/4091 [00:35<00:03, 102.80it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  90%|▉| 3696/4091 [00:36<00:03, 101.34it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  91%|▉| 3724/4091 [00:36<00:03, 101.80it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  92%|▉| 3752/4091 [00:36<00:03, 102.25it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  92%|▉| 3780/4091 [00:36<00:03, 102.68it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  93%|▉| 3808/4091 [00:36<00:02, 103.13it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  94%|▉| 3836/4091 [00:37<00:02, 103.57it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 200.39it/s]\u001b[A\n",
      "Epoch 10:  94%|▉| 3864/4091 [00:37<00:02, 103.99it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  95%|▉| 3892/4091 [00:37<00:01, 104.42it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  96%|▉| 3920/4091 [00:37<00:01, 104.86it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  97%|▉| 3948/4091 [00:37<00:01, 105.29it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  97%|▉| 3976/4091 [00:37<00:01, 105.72it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  98%|▉| 4004/4091 [00:37<00:00, 106.14it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  99%|▉| 4032/4091 [00:37<00:00, 106.57it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 10:  99%|▉| 4060/4091 [00:37<00:00, 106.99it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Validating:  93%|█████████████████████████▉  | 375/404 [00:02<00:00, 245.38it/s]\u001b[A\n",
      "Epoch 10: 100%|█| 4091/4091 [00:38<00:00, 107.18it/s, loss=5.02, v_num=0, loss/v\u001b[A\n",
      "Epoch 11:  90%|▉| 3687/4091 [00:35<00:03, 103.29it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  90%|▉| 3696/4091 [00:36<00:03, 101.86it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  91%|▉| 3724/4091 [00:36<00:03, 102.31it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  92%|▉| 3752/4091 [00:36<00:03, 102.76it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  92%|▉| 3780/4091 [00:36<00:03, 103.22it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  93%|▉| 3808/4091 [00:36<00:02, 103.67it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  94%|▉| 3836/4091 [00:36<00:02, 104.11it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  94%|▉| 3864/4091 [00:36<00:02, 104.55it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Validating:  44%|████████████▎               | 178/404 [00:01<00:01, 216.78it/s]\u001b[A\n",
      "Epoch 11:  95%|▉| 3892/4091 [00:37<00:01, 104.98it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  96%|▉| 3920/4091 [00:37<00:01, 105.42it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  97%|▉| 3948/4091 [00:37<00:01, 105.83it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  97%|▉| 3976/4091 [00:37<00:01, 106.26it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  98%|▉| 4004/4091 [00:37<00:00, 106.68it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  99%|▉| 4032/4091 [00:37<00:00, 107.10it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11:  99%|▉| 4060/4091 [00:37<00:00, 107.52it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11: 100%|▉| 4088/4091 [00:37<00:00, 107.93it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 11: 100%|█| 4091/4091 [00:37<00:00, 107.71it/s, loss=5, v_num=0, loss/vali\u001b[A\n",
      "Epoch 12:  90%|▉| 3687/4091 [00:35<00:03, 103.32it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  90%|▉| 3696/4091 [00:36<00:03, 101.87it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  91%|▉| 3724/4091 [00:36<00:03, 102.33it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  92%|▉| 3752/4091 [00:36<00:03, 102.77it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  92%|▉| 3780/4091 [00:36<00:03, 103.21it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  93%|▉| 3808/4091 [00:36<00:02, 103.64it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  94%|▉| 3836/4091 [00:36<00:02, 104.09it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▎                 | 149/404 [00:01<00:01, 199.79it/s]\u001b[A\n",
      "Epoch 12:  94%|▉| 3864/4091 [00:36<00:02, 104.52it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  95%|▉| 3892/4091 [00:37<00:01, 104.96it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  96%|▉| 3920/4091 [00:37<00:01, 105.39it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  97%|▉| 3948/4091 [00:37<00:01, 105.82it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  97%|▉| 3976/4091 [00:37<00:01, 106.25it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  98%|▉| 4004/4091 [00:37<00:00, 106.68it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  99%|▉| 4032/4091 [00:37<00:00, 107.10it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 12:  99%|▉| 4060/4091 [00:37<00:00, 107.52it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Validating:  93%|█████████████████████████▉  | 374/404 [00:02<00:00, 244.64it/s]\u001b[A\n",
      "Epoch 12: 100%|█| 4091/4091 [00:37<00:00, 107.72it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  90%|▉| 3687/4091 [00:35<00:03, 103.51it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  90%|▉| 3696/4091 [00:36<00:03, 102.06it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  91%|▉| 3724/4091 [00:36<00:03, 102.54it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  92%|▉| 3752/4091 [00:36<00:03, 102.99it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  92%|▉| 3780/4091 [00:36<00:03, 103.44it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  93%|▉| 3808/4091 [00:36<00:02, 103.88it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  94%|▉| 3836/4091 [00:36<00:02, 104.32it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  94%|▉| 3864/4091 [00:36<00:02, 104.76it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Validating:  44%|████████████▎               | 177/404 [00:01<00:01, 216.26it/s]\u001b[A\n",
      "Epoch 13:  95%|▉| 3892/4091 [00:36<00:01, 105.19it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  96%|▉| 3920/4091 [00:37<00:01, 105.63it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  97%|▉| 3948/4091 [00:37<00:01, 106.06it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  97%|▉| 3976/4091 [00:37<00:01, 106.48it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  98%|▉| 4004/4091 [00:37<00:00, 106.90it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  99%|▉| 4032/4091 [00:37<00:00, 107.32it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13:  99%|▉| 4060/4091 [00:37<00:00, 107.74it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13: 100%|▉| 4088/4091 [00:37<00:00, 108.17it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 13: 100%|█| 4091/4091 [00:37<00:00, 107.96it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  90%|▉| 3687/4091 [00:35<00:03, 102.71it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  90%|▉| 3696/4091 [00:36<00:03, 101.30it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  91%|▉| 3724/4091 [00:36<00:03, 101.75it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  92%|▉| 3752/4091 [00:36<00:03, 102.19it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  92%|▉| 3780/4091 [00:36<00:03, 102.63it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  93%|▉| 3808/4091 [00:36<00:02, 103.07it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  94%|▉| 3836/4091 [00:37<00:02, 103.51it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 201.32it/s]\u001b[A\n",
      "Epoch 14:  94%|▉| 3864/4091 [00:37<00:02, 103.95it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  95%|▉| 3892/4091 [00:37<00:01, 104.38it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  96%|▉| 3920/4091 [00:37<00:01, 104.81it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  97%|▉| 3948/4091 [00:37<00:01, 105.24it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  97%|▉| 3976/4091 [00:37<00:01, 105.66it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  98%|▉| 4004/4091 [00:37<00:00, 106.08it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  99%|▉| 4032/4091 [00:37<00:00, 106.50it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14:  99%|▉| 4060/4091 [00:37<00:00, 106.92it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14: 100%|▉| 4088/4091 [00:38<00:00, 107.35it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 14: 100%|█| 4091/4091 [00:38<00:00, 107.13it/s, loss=4.95, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  90%|▉| 3687/4091 [00:35<00:03, 103.05it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  90%|▉| 3696/4091 [00:36<00:03, 101.63it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  91%|▉| 3724/4091 [00:36<00:03, 102.08it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  92%|▉| 3752/4091 [00:36<00:03, 102.52it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  92%|▉| 3780/4091 [00:36<00:03, 102.97it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  93%|▉| 3808/4091 [00:36<00:02, 103.40it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  94%|▉| 3836/4091 [00:36<00:02, 103.84it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▎                 | 149/404 [00:01<00:01, 200.98it/s]\u001b[A\n",
      "Epoch 15:  94%|▉| 3864/4091 [00:37<00:02, 104.28it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  95%|▉| 3892/4091 [00:37<00:01, 104.71it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  96%|▉| 3920/4091 [00:37<00:01, 105.14it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  97%|▉| 3948/4091 [00:37<00:01, 105.57it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  97%|▉| 3976/4091 [00:37<00:01, 106.00it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  98%|▉| 4004/4091 [00:37<00:00, 106.42it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  99%|▉| 4032/4091 [00:37<00:00, 106.83it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15:  99%|▉| 4060/4091 [00:37<00:00, 107.25it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15: 100%|▉| 4088/4091 [00:37<00:00, 107.68it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 15: 100%|█| 4091/4091 [00:38<00:00, 107.47it/s, loss=4.92, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  90%|▉| 3687/4091 [00:35<00:03, 103.56it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  90%|▉| 3696/4091 [00:36<00:03, 102.13it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  91%|▉| 3724/4091 [00:36<00:03, 102.58it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  92%|▉| 3752/4091 [00:36<00:03, 103.02it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  92%|▉| 3780/4091 [00:36<00:03, 103.46it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  93%|▉| 3808/4091 [00:36<00:02, 103.90it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  94%|▉| 3836/4091 [00:36<00:02, 104.34it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  94%|▉| 3864/4091 [00:36<00:02, 104.78it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Validating:  44%|████████████▎               | 177/404 [00:01<00:01, 213.41it/s]\u001b[A\n",
      "Epoch 16:  95%|▉| 3892/4091 [00:36<00:01, 105.20it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  96%|▉| 3920/4091 [00:37<00:01, 105.64it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  97%|▉| 3948/4091 [00:37<00:01, 106.07it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  97%|▉| 3976/4091 [00:37<00:01, 106.49it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  98%|▉| 4004/4091 [00:37<00:00, 106.92it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  99%|▉| 4032/4091 [00:37<00:00, 107.34it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16:  99%|▉| 4060/4091 [00:37<00:00, 107.75it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16: 100%|▉| 4088/4091 [00:37<00:00, 108.18it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 16: 100%|█| 4091/4091 [00:37<00:00, 107.94it/s, loss=4.96, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  90%|▉| 3687/4091 [00:35<00:03, 103.69it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  90%|▉| 3696/4091 [00:36<00:03, 102.23it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  91%|▉| 3724/4091 [00:36<00:03, 102.70it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  92%|▉| 3752/4091 [00:36<00:03, 103.14it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  92%|▉| 3780/4091 [00:36<00:03, 103.59it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  93%|▉| 3808/4091 [00:36<00:02, 104.04it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  94%|▉| 3836/4091 [00:36<00:02, 104.47it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▍                 | 151/404 [00:01<00:01, 200.43it/s]\u001b[A\n",
      "Epoch 17:  94%|▉| 3864/4091 [00:36<00:02, 104.90it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  95%|▉| 3892/4091 [00:36<00:01, 105.33it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  96%|▉| 3920/4091 [00:37<00:01, 105.75it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  97%|▉| 3948/4091 [00:37<00:01, 106.18it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  97%|▉| 3976/4091 [00:37<00:01, 106.61it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  98%|▉| 4004/4091 [00:37<00:00, 107.03it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  99%|▉| 4032/4091 [00:37<00:00, 107.46it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17:  99%|▉| 4060/4091 [00:37<00:00, 107.87it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17: 100%|▉| 4088/4091 [00:37<00:00, 108.30it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 17: 100%|█| 4091/4091 [00:37<00:00, 108.10it/s, loss=4.93, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  90%|▉| 3687/4091 [00:35<00:03, 103.94it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 18:  90%|▉| 3696/4091 [00:36<00:03, 102.47it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  91%|▉| 3724/4091 [00:36<00:03, 102.92it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  92%|▉| 3752/4091 [00:36<00:03, 103.36it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  92%|▉| 3780/4091 [00:36<00:02, 103.80it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  93%|▉| 3808/4091 [00:36<00:02, 104.25it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  94%|▉| 3836/4091 [00:36<00:02, 104.69it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 200.87it/s]\u001b[A\n",
      "Epoch 18:  94%|▉| 3864/4091 [00:36<00:02, 105.12it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  95%|▉| 3892/4091 [00:36<00:01, 105.56it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  96%|▉| 3920/4091 [00:36<00:01, 105.99it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  97%|▉| 3948/4091 [00:37<00:01, 106.42it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  97%|▉| 3976/4091 [00:37<00:01, 106.84it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  98%|▉| 4004/4091 [00:37<00:00, 107.27it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  99%|▉| 4032/4091 [00:37<00:00, 107.69it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18:  99%|▉| 4060/4091 [00:37<00:00, 108.12it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18: 100%|▉| 4088/4091 [00:37<00:00, 108.54it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 18: 100%|█| 4091/4091 [00:37<00:00, 108.32it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  90%|▉| 3687/4091 [00:35<00:03, 102.75it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                       | 0/404 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  90%|▉| 3696/4091 [00:36<00:03, 101.31it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  91%|▉| 3724/4091 [00:36<00:03, 101.77it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  92%|▉| 3752/4091 [00:36<00:03, 102.21it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  92%|▉| 3780/4091 [00:36<00:03, 102.64it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  93%|▉| 3808/4091 [00:36<00:02, 103.09it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  94%|▉| 3836/4091 [00:37<00:02, 103.52it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Validating:  37%|██████████▍                 | 150/404 [00:01<00:01, 198.34it/s]\u001b[A\n",
      "Epoch 19:  94%|▉| 3864/4091 [00:37<00:02, 103.94it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  95%|▉| 3892/4091 [00:37<00:01, 104.37it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  96%|▉| 3920/4091 [00:37<00:01, 104.81it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  97%|▉| 3948/4091 [00:37<00:01, 105.24it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  97%|▉| 3976/4091 [00:37<00:01, 105.66it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  98%|▉| 4004/4091 [00:37<00:00, 106.09it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  99%|▉| 4032/4091 [00:37<00:00, 106.51it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19:  99%|▉| 4060/4091 [00:37<00:00, 106.93it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19: 100%|▉| 4088/4091 [00:38<00:00, 107.35it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19: 100%|█| 4091/4091 [00:38<00:00, 107.13it/s, loss=4.94, v_num=0, loss/v\u001b[A\n",
      "Epoch 19: 100%|█| 4091/4091 [00:38<00:00, 107.03it/s, loss=4.94, v_num=0, loss/v\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  97%|██████████████████████████████▏| 451/463 [00:02<00:00, 245.98it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'loss/test': 4.6080121994018555, 'perplexity/test': 102.39926147460938}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|███████████████████████████████| 463/463 [00:02<00:00, 183.32it/s]\n",
      "[{'loss/test': 4.6080121994018555, 'perplexity/test': 102.39926147460938}]\n"
     ]
    }
   ],
   "source": [
    "# Train RNN\n",
    "vocab_size = len(word_freqs)\n",
    "data_module = TextDateModule(train, valid, test)\n",
    "model = TextLightningModule(vocab_size)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(\"./lightning_logs/\", name=\"network_1\")\n",
    "trainer = pl.Trainer(logger=tb_logger, max_epochs=20, gpus=1)\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "result = trainer.test(model, data_module)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f50902-6c35-48b3-9dbd-dc548b7655d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTMModule(pl.LightningModule):\n",
    "    \"\"\"LSTM modeule.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = 2\n",
    "        self.hidden_size = 100 #200\n",
    "        self.embedding_size = 100\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        #layers\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.out_fc = nn.Linear(self.hidden_size, vocab_size)\n",
    "        # loss funciton\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    \n",
    "    def forward(self, data, hidden, cell):\n",
    "        embedding = self.dropout(self.embedding(data))\n",
    "        output, hidden = self.lstm(embedding, (hidden, cell))\n",
    "        output = self.out_fc(output)\n",
    "        return output.view(-1, self.vocab_size), (hidden, cell)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=5)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, (hidden, cell) = self.forward(x, hidden, cell)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'perplexity': {'train': perplexity}, 'loss': {'train': loss.detach()}}\n",
    "        self.log(\"loss/train\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"perplexity/train\", perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, (hidden, cell) = self.forward(x, hidden, cell)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'perplexity': {'valid': perplexity}, 'loss': {'valid': loss.detach()}}\n",
    "        self.log(\"loss/valid\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"perplexity/valid\", perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, (hidden, cell) = self.forward(x, hidden, cell)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "        \n",
    "        tensorboard_logs = {'perplexity': {'test': perplexity}, 'loss': {'test': loss.detach()}}\n",
    "        self.log(\"loss/test\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"perplexity/test\", perplexity, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def init_hidden(self, batch_size = 20):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a88057e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit as jit\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from torch.nn import Parameter\n",
    "from collections import namedtuple\n",
    "LSTMState = namedtuple(\"LSTMState\", [\"hx\", \"cx\"])\n",
    "\n",
    "class LSTMCell(jit.ScriptModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_ih = Parameter(torch.randn(4 * hidden_size, input_size))\n",
    "        self.weight_hh = Parameter(torch.randn(4 * hidden_size, hidden_size))\n",
    "        self.bias_ih = Parameter(torch.randn(4 * hidden_size))\n",
    "        self.bias_hh = Parameter(torch.randn(4 * hidden_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    @jit.script_method\n",
    "    def forward(\n",
    "        self, input: Tensor, state: Tuple[Tensor, Tensor]\n",
    "    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        hx, cx = state\n",
    "        gates = (\n",
    "            torch.mm(input, self.weight_ih.t())\n",
    "            + self.bias_ih\n",
    "            + torch.mm(hx, self.weight_hh.t())\n",
    "            + self.bias_hh\n",
    "        )\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy)\n",
    "\n",
    "\n",
    "class LSTMLayer(jit.ScriptModule):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "\n",
    "    @jit.script_method\n",
    "    def forward(\n",
    "        self, input: Tensor, state: Tuple[Tensor, Tensor]\n",
    "    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        inputs = input.unbind(1)\n",
    "        outputs = torch.jit.annotate(List[Tensor], [])\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs,1), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15717dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMState = namedtuple('LSTMState', ['hx', 'cx'])\n",
    "class TextLSTMModule(pl.LightningModule):\n",
    "    \"\"\"LSTM modeule.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 100  # 200\n",
    "        self.embedding_size = 100\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # embedding\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        # layers\n",
    "        # self.lstm = nn.LSTM(\n",
    "        #     self.embedding_size, self.hidden_size, self.num_layers, batch_first=True\n",
    "        # )\n",
    "        self.lstm = LSTMLayer(LSTMCell, self.embedding_size, self.hidden_size)\n",
    "        self.out_fc = nn.Linear(self.hidden_size, vocab_size)\n",
    "        # loss funciton\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, data, hidden, cell):\n",
    "        embedding = self.dropout(self.embedding(data))\n",
    "        state = LSTMState(hidden,cell)\n",
    "        output, hidden = self.lstm(embedding, state)\n",
    "        output = self.out_fc(output)\n",
    "        return output.view(-1, self.vocab_size), (hidden, cell)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=5)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(-1)\n",
    "\n",
    "        hidden = torch.zeros( 20, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(20, self.hidden_size).to(self.device)\n",
    "        output, (hidden, cell) = self.forward(x, hidden, cell)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"perplexity\": {\"train\": perplexity},\n",
    "            \"loss\": {\"train\": loss.detach()},\n",
    "        }\n",
    "        self.log(\n",
    "            \"loss/train\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"perplexity/train\",\n",
    "            perplexity,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(-1)\n",
    "\n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, (hidden, cell) = self.forward(x, hidden, cell)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"perplexity\": {\"valid\": perplexity},\n",
    "            \"loss\": {\"valid\": loss.detach()},\n",
    "        }\n",
    "        self.log(\n",
    "            \"loss/valid\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"perplexity/valid\",\n",
    "            perplexity,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(-1)\n",
    "\n",
    "        hidden = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        cell = torch.zeros(self.num_layers, 20, self.hidden_size).to(self.device)\n",
    "        output, (hidden, cell) = self.forward(x, hidden, cell)\n",
    "        loss = self.loss(output, y)\n",
    "        perplexity = math.exp(loss.item())\n",
    "\n",
    "        tensorboard_logs = {\n",
    "            \"perplexity\": {\"test\": perplexity},\n",
    "            \"loss\": {\"test\": loss.detach()},\n",
    "        }\n",
    "        self.log(\n",
    "            \"loss/test\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        self.log(\n",
    "            \"perplexity/test\",\n",
    "            perplexity,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def init_hidden(self, batch_size=20):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b402e09f-e879-4a63-8c70-5c4778be3317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | embedding | Embedding        | 2.9 M \n",
      "1 | lstm      | LSTMLayer        | 80.8 K\n",
      "2 | out_fc    | Linear           | 2.9 M \n",
      "3 | loss      | CrossEntropyLoss | 0     \n",
      "4 | dropout   | Dropout          | 0     \n",
      "-----------------------------------------------\n",
      "5.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.9 M     Total params\n",
      "23.561    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4b1306df1a4cce999c29c27709e2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train LSTM\n",
    "vocab_size = len(word_freqs)\n",
    "lstm_data_module = TextDateModule(train, valid, test)\n",
    "lstm_model = TextLSTMModule(vocab_size)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger(\"./lightning_logs/\", name=\"network_2\")\n",
    "trainer = pl.Trainer(logger=tb_logger, gradient_clip_val=0.5, max_epochs=20, gpus=1)\n",
    "trainer.fit(lstm_model, lstm_data_module)\n",
    "\n",
    "result = trainer.test(lstm_model, lstm_data_module)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7442223-3d62-4514-b1ff-d840601d87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
